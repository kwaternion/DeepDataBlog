{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "TODO:\n",
    "* Add graphs with few points\n",
    "* Color points\n",
    "* add perceptron classifier with few realizations (also optimal)\n",
    "\n",
    "Assumption: dataset is linearly separable.\n",
    "\n",
    "As it can be seen classifier with separating line (hyperplane in general) which is furthest to classified points should be best. Such classifier should be least sensitive to small variations to the points (keeping same label) if points are disturbed by some noise. \n",
    "Such classifier is called maximum margin classifier.\n",
    "\n",
    "For given parameters set: $\\mathbf{w} \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$, classification is made accordingly:\n",
    "$h(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^T\\mathbf{x}+b)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating hyperplane:\n",
    "\n",
    "The hyperplane is separates the dataset if there exists such $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ that the following condition is fulfilled:\n",
    "\n",
    "\\begin{equation} \\label{eq:inequ_cond} \n",
    "y_n(\\mathbf{w}^T \\mathbf{x_n}+b) > 0\n",
    "\\end{equation}\n",
    "\n",
    "Magnitude of such inequality is irrelevant. It can be shown that that multiplication of both parameters by scalar ($\\mathbf{w}/\\rho, b/\\rho$) does not change the above condition.\n",
    "Especially, the smallest value fulfilling condition \\ref{eq:inequ_cond} can be found:\n",
    "\n",
    "\\begin{equation} \\label{eq:inequ_cond2} \n",
    "\\rho = \\min_{n = 1,..,N} y_n(\\mathbf{w}^T \\mathbf{x_n}+b)\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Such value is positive and $\\rho >0$ (due to \\ref{eq:inequ_cond} condition). Then the weights of maximal margin separator could be rescaled such $\\rho$ value ($\\mathbf{w}/\\rho, b/\\rho$).\n",
    "Such rescaling modifies eq. \\ref{eq:inequ_cond} accordingly:\n",
    "\n",
    "\\begin{equation} \\label{eq:inequ_cond3} \n",
    "\\rho = \\min_{n = 1,..,N} y_n(\\mathbf{w/\\rho}^T \\mathbf{x_n}+b/\\rho) = 1/\\rho \\min_{n = 1,..,N} y_n(\\mathbf{w}^T \\mathbf{x_n}+b) = \\frac{\\rho}{\\rho} = 1\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Definition: \n",
    "Separating hyperplane is such plane that there exist such $\\mathbf{w}$ and $b$ that the following condition is satisfied:\n",
    "\\begin{equation} \\label{eq:inequ_cond4} \n",
    "\\min_{n = 1,..,N} y_n(\\mathbf{w}^T \\mathbf{x_n}+b) = 1\n",
    "\\end{equation}\n",
    "\n",
    "Just to mention: such condition dose not guarantee optimum of the separating hyperplane. It just assures that such hyperplane will properly classify training points (under assumption of linear separability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width of the separation margin\n",
    "\n",
    "In order to optimize separating hyperplane, such that the distance between points and decision boundary is maximized, we need to introduce a way of calculation distance between any arbitrary point and hyperplane.\n",
    "Lest start by calculating distance to any arbitrary point $\\mathbf{x}$ to separating hyperplane $h(\\mathbf{w},b)$.\n",
    "Such distance will be denoted as: $\\mathrm{dist}(\\mathbf{x},b)$. Such distance will be minimized if distance $\\mathrm{dist}$ is perpendicular to the separating hyperplane.\n",
    "For any point $\\mathbf{x'}$ which is located on hyperplane the following condition is meet: $\\mathbf{w}^T \\mathbf{x'}+b = 0$. Let the $\\mathbf{u}$ be unit vector, perpendicular to hyperplane, then the distance can be expressed as:\n",
    "\n",
    "\\begin{equation} \\label{eq:dist_plane} \n",
    "\\mathrm{dist}(\\mathbf{x},b) = |\\mathbf{u^T}(\\mathbf{x'} - \\mathbf{x})|\n",
    "\\end{equation}\n",
    "\n",
    "which can be understood as a projection of vector $(\\mathbf{x'} - \\mathbf{x})$ onto vector $\\mathbf{u}$, which is perpendicular to the hyperplane. We don't know yet how to express $\\mathbf{u}$ vector, but we will do it soon.\n",
    "\n",
    "Now we will show that vector $\\mathbf{w}$ is perpendicular to the separating hyperplane. We will do it by showing demonstrating that any vector that is located on the hyperplane is perpendicular to the weights vector. Lets construct a vector on the hyperplane, this can be achieved by introducing another arbitrary chosen point on the hyperplane $\\mathbf{x''}$. Since both points $\\mathbf{x'}$ and $\\mathbf{x''}$ are located on the hyperplane, thus the vector $\\mathbf{x'}- \\mathbf{x''}$ must be laying on the hyperplane. Now we will show that such vector is perpendicular to the $\\mathbf{w}$ vector. This can be achieved by calculating scalar product between those two vectors:\n",
    "\n",
    "\\begin{equation} \\label{eq:inequ_cond4} \n",
    "\\mathbf{w^T}(\\mathbf{x'}- \\mathbf{x''}) = \\mathbf{w^T}\\mathbf{x'} - \\mathbf{w^T}\\mathbf{x''} = b -b = 0\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Since scalar product equals to zero, both vectors are perpendicular to each other, which implies that $\\mathbf{w}$ vector is perpendicular to the separating plane.\n",
    " \n",
    "So now we can easily construct unit vector $\\mathbf{u}$, which is perpendicular to the separating hyperplane:\n",
    "\\begin{equation} \n",
    "\\mathbf{u} = \\mathbf{w}/\\|\\mathbf{w}\\|\n",
    "\\nonumber\n",
    "\\end{equation}\n",
    "\n",
    "Now the distance equation \\ref{eq:dist_plane} can be expressed accordingly:\n",
    "\n",
    "\\begin{equation} \\label{eq:dist_plane2} \n",
    "\\mathrm{dist}(\\mathbf{x},b) = |\\mathbf{u^T}(\\mathbf{x'} - \\mathbf{x})| = \\frac{|\\mathbf{w^T}\\mathbf{x} - \\mathbf{w^T}\\mathbf{x'}|}{\\|\\mathbf{w}\\|} =  \\frac{|\\mathbf{w^T}\\mathbf{x} +b|}{\\|\\mathbf{w}\\|}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36ds",
   "language": "python",
   "name": "py36ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
